{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['abstract', 'articleID', 'articleWordCount', 'byline', 'documentType',\n",
      "       'headline', 'keywords', 'multimedia', 'newDesk', 'printPage', 'pubDate',\n",
      "       'sectionName', 'snippet', 'source', 'typeOfMaterial', 'webURL'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "\n",
    "df = pd.read_csv(\"data/ArticlesApril2017.csv\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class TextGeneration(Dataset):\n",
    "    def clean_text(self, txt):\n",
    "        txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "        return txt\n",
    "    \n",
    "    def __init__(self):\n",
    "        all_headlines = []\n",
    "        \n",
    "        for filename in glob.glob(\"data/*.csv\"):\n",
    "            if 'Articles' in filename:\n",
    "                article_df = pd.read_csv(filename)\n",
    "                \n",
    "                all_headlines.extend(list(article_df.headline.values))\n",
    "                break\n",
    "        \n",
    "        all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
    "        \n",
    "        self.corpus = [self.clean_text(x) for x in all_headlines]\n",
    "        self.BOW = {}\n",
    "        \n",
    "        for line in self.corpus:\n",
    "            for word in line.split():\n",
    "                if word not in self.BOW.keys():\n",
    "                    self.BOW[word] = len(self.BOW.keys())\n",
    "        \n",
    "        self.data = self.generate_sequence(self.corpus)\n",
    "    \n",
    "    def generate_sequence(self, txt):\n",
    "        seq = []\n",
    "        \n",
    "        for line in txt:\n",
    "            line = line.split()\n",
    "            line_bow = [self.BOW[word] for word in line]\n",
    "            \n",
    "            data = [([line_bow[i], line_bow[i+1]], line_bow[i+2]) for i in range(len(line_bow)-2)]\n",
    "            \n",
    "            seq.extend(data)\n",
    "        \n",
    "        return seq\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        data = np.array(self.data[i][0])\n",
    "        label = np.array(self.data[i][1]).astype(np.float32)\n",
    "        \n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self.BOW는 Bag Of Words로, 모든 단어를 겹치지 않도록 고유번호로 나타낸 집합을 뜻한다. generate_sequence는 인접한 두 단어를 입력 데이터로, 그 다음에 올 단어를 정답으로 사용하는 text sequence를 만들어주는 함수이다.\n",
    "\n",
    "dataset = TextGeneration()일 때, __init__에서 정의한 요소를 출력하면 다음과 같다.\n",
    "\n",
    "- dataset.BOW = {'i': 0, 'stand': 1, 'with': 2, 'the': 3, '‘shedevils’': 4, 'trump’s': 5, 'birth': 6, ...}\n",
    "\n",
    "- dataset.corpus = ['i stand  with the ‘shedevils’', 'trump’s birth control problems', ...]\n",
    "\n",
    "- dataset.data = [([0, 1], 2), ([1, 2], 3), ([2, 3], 4), ([5, 6], 7), ([6, 7], 8), ([9, 3], 10), ([3, 10], 11), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_embeddings):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=16)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=16, hidden_size=64, num_layers=5, batch_first=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128, num_embeddings)\n",
    "        self.fc2 = nn.Linear(num_embeddings, num_embeddings)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        \n",
    "        x, _ = self.lstm(x)\n",
    "        x = torch.reshape(x, (x.shape[0], -1))\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM의 input은 vocabulary data 2개 * batch_size (64)이다. embed를 통해 vocabulary data 2개는 16차원 벡터 2개로 바뀌고, lstm을 통해 과거의 정보를 포함하는 64차원 벡터 2개로 변환된다. 이후 reshape를 통해 128차원 벡터 1개로 바뀌고, linear transformation과 relu 층을 거쳐 num_embeddings (3214)차원 벡터 1개가 반환된다. 즉, 최종적으로는 3214차원 벡터가 batch_size (64)개만큼 반환되는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/88 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "epoch0 loss:8.012681007385254: 100%|██████████| 88/88 [00:00<00:00, 98.18it/s] \n",
      "epoch1 loss:7.663285732269287: 100%|██████████| 88/88 [00:00<00:00, 94.52it/s] \n",
      "epoch2 loss:7.365994453430176: 100%|██████████| 88/88 [00:00<00:00, 102.57it/s] \n",
      "epoch3 loss:7.0153422355651855: 100%|██████████| 88/88 [00:00<00:00, 103.09it/s]\n",
      "epoch4 loss:6.634801864624023: 100%|██████████| 88/88 [00:00<00:00, 102.87it/s] \n",
      "epoch5 loss:6.116247177124023: 100%|██████████| 88/88 [00:00<00:00, 102.56it/s] \n",
      "epoch6 loss:6.033440589904785: 100%|██████████| 88/88 [00:00<00:00, 101.91it/s] \n",
      "epoch7 loss:5.853763580322266: 100%|██████████| 88/88 [00:00<00:00, 102.52it/s] \n",
      "epoch8 loss:5.569308280944824: 100%|██████████| 88/88 [00:00<00:00, 102.00it/s] \n",
      "epoch9 loss:6.205223560333252: 100%|██████████| 88/88 [00:00<00:00, 101.75it/s] \n",
      "epoch10 loss:7.632265090942383: 100%|██████████| 88/88 [00:00<00:00, 101.12it/s] \n",
      "epoch11 loss:6.902897357940674: 100%|██████████| 88/88 [00:00<00:00, 100.49it/s] \n",
      "epoch12 loss:7.356176376342773: 100%|██████████| 88/88 [00:00<00:00, 100.46it/s] \n",
      "epoch13 loss:6.084029674530029: 100%|██████████| 88/88 [00:00<00:00, 100.54it/s] \n",
      "epoch14 loss:6.901860237121582: 100%|██████████| 88/88 [00:00<00:00, 100.37it/s] \n",
      "epoch15 loss:7.1816182136535645: 100%|██████████| 88/88 [00:00<00:00, 100.27it/s]\n",
      "epoch16 loss:6.670312404632568: 100%|██████████| 88/88 [00:00<00:00, 100.46it/s] \n",
      "epoch17 loss:7.167685508728027: 100%|██████████| 88/88 [00:00<00:00, 99.58it/s]  \n",
      "epoch18 loss:7.234215259552002: 100%|██████████| 88/88 [00:00<00:00, 100.43it/s] \n",
      "epoch19 loss:7.074178695678711: 100%|██████████| 88/88 [00:00<00:00, 100.37it/s] \n",
      "epoch20 loss:6.992053985595703: 100%|██████████| 88/88 [00:00<00:00, 96.63it/s]  \n",
      "epoch21 loss:6.919243812561035: 100%|██████████| 88/88 [00:00<00:00, 100.30it/s] \n",
      "epoch22 loss:6.868431568145752: 100%|██████████| 88/88 [00:00<00:00, 99.64it/s]  \n",
      "epoch23 loss:6.821829795837402: 100%|██████████| 88/88 [00:00<00:00, 100.55it/s] \n",
      "epoch24 loss:6.728602409362793: 100%|██████████| 88/88 [00:00<00:00, 100.49it/s] \n",
      "epoch25 loss:6.699217319488525: 100%|██████████| 88/88 [00:00<00:00, 100.34it/s] \n",
      "epoch26 loss:6.663362503051758: 100%|██████████| 88/88 [00:00<00:00, 99.65it/s]  \n",
      "epoch27 loss:6.5913286209106445: 100%|██████████| 88/88 [00:00<00:00, 98.11it/s] \n",
      "epoch28 loss:6.533686637878418: 100%|██████████| 88/88 [00:00<00:00, 97.93it/s] \n",
      "epoch29 loss:6.461575984954834: 100%|██████████| 88/88 [00:00<00:00, 100.57it/s] \n",
      "epoch30 loss:6.394367694854736: 100%|██████████| 88/88 [00:00<00:00, 100.49it/s] \n",
      "epoch31 loss:6.286355495452881: 100%|██████████| 88/88 [00:00<00:00, 100.69it/s] \n",
      "epoch32 loss:6.311563491821289: 100%|██████████| 88/88 [00:00<00:00, 98.52it/s]  \n",
      "epoch33 loss:6.215634822845459: 100%|██████████| 88/88 [00:00<00:00, 98.36it/s]  \n",
      "epoch34 loss:6.168679237365723: 100%|██████████| 88/88 [00:00<00:00, 98.49it/s] \n",
      "epoch35 loss:6.102522373199463: 100%|██████████| 88/88 [00:00<00:00, 100.70it/s] \n",
      "epoch36 loss:6.0361857414245605: 100%|██████████| 88/88 [00:00<00:00, 100.34it/s]\n",
      "epoch37 loss:5.94985818862915: 100%|██████████| 88/88 [00:00<00:00, 100.46it/s]  \n",
      "epoch38 loss:6.146020889282227: 100%|██████████| 88/88 [00:00<00:00, 100.38it/s] \n",
      "epoch39 loss:6.015742778778076: 100%|██████████| 88/88 [00:00<00:00, 99.95it/s]  \n",
      "epoch40 loss:5.976992130279541: 100%|██████████| 88/88 [00:00<00:00, 100.38it/s] \n",
      "epoch41 loss:5.962806224822998: 100%|██████████| 88/88 [00:00<00:00, 100.35it/s] \n",
      "epoch42 loss:5.831970691680908: 100%|██████████| 88/88 [00:00<00:00, 100.50it/s] \n",
      "epoch43 loss:5.791224956512451: 100%|██████████| 88/88 [00:00<00:00, 100.35it/s] \n",
      "epoch44 loss:5.695001602172852: 100%|██████████| 88/88 [00:00<00:00, 100.29it/s] \n",
      "epoch45 loss:5.705836772918701: 100%|██████████| 88/88 [00:00<00:00, 100.44it/s] \n",
      "epoch46 loss:5.630878925323486: 100%|██████████| 88/88 [00:00<00:00, 100.41it/s] \n",
      "epoch47 loss:5.710471153259277: 100%|██████████| 88/88 [00:00<00:00, 100.41it/s] \n",
      "epoch48 loss:5.679805278778076: 100%|██████████| 88/88 [00:00<00:00, 100.26it/s] \n",
      "epoch49 loss:5.623770236968994: 100%|██████████| 88/88 [00:00<00:00, 100.35it/s] \n",
      "epoch50 loss:5.495511531829834: 100%|██████████| 88/88 [00:00<00:00, 100.40it/s] \n",
      "epoch51 loss:5.417269706726074: 100%|██████████| 88/88 [00:00<00:00, 100.33it/s] \n",
      "epoch52 loss:5.305499076843262: 100%|██████████| 88/88 [00:00<00:00, 100.38it/s] \n",
      "epoch53 loss:5.149134635925293: 100%|██████████| 88/88 [00:00<00:00, 100.33it/s] \n",
      "epoch54 loss:4.962045669555664: 100%|██████████| 88/88 [00:00<00:00, 100.43it/s] \n",
      "epoch55 loss:4.946308612823486: 100%|██████████| 88/88 [00:00<00:00, 100.31it/s] \n",
      "epoch56 loss:4.828677654266357: 100%|██████████| 88/88 [00:00<00:00, 100.44it/s] \n",
      "epoch57 loss:4.747101783752441: 100%|██████████| 88/88 [00:00<00:00, 100.29it/s] \n",
      "epoch58 loss:4.700966835021973: 100%|██████████| 88/88 [00:00<00:00, 99.45it/s]  \n",
      "epoch59 loss:4.840112209320068: 100%|██████████| 88/88 [00:00<00:00, 100.03it/s] \n",
      "epoch60 loss:4.693832874298096: 100%|██████████| 88/88 [00:00<00:00, 100.19it/s] \n",
      "epoch61 loss:4.640974521636963: 100%|██████████| 88/88 [00:00<00:00, 102.43it/s] \n",
      "epoch62 loss:4.603885173797607: 100%|██████████| 88/88 [00:00<00:00, 101.58it/s] \n",
      "epoch63 loss:4.624929904937744: 100%|██████████| 88/88 [00:00<00:00, 102.77it/s] \n",
      "epoch64 loss:4.497870922088623: 100%|██████████| 88/88 [00:00<00:00, 102.88it/s] \n",
      "epoch65 loss:4.525157928466797: 100%|██████████| 88/88 [00:00<00:00, 102.46it/s] \n",
      "epoch66 loss:4.518599987030029: 100%|██████████| 88/88 [00:00<00:00, 100.40it/s] \n",
      "epoch67 loss:4.421214580535889: 100%|██████████| 88/88 [00:00<00:00, 100.16it/s] \n",
      "epoch68 loss:4.211602687835693: 100%|██████████| 88/88 [00:00<00:00, 100.26it/s] \n",
      "epoch69 loss:4.513842582702637: 100%|██████████| 88/88 [00:00<00:00, 100.48it/s] \n",
      "epoch70 loss:4.546356201171875: 100%|██████████| 88/88 [00:00<00:00, 100.48it/s] \n",
      "epoch71 loss:4.470378398895264: 100%|██████████| 88/88 [00:00<00:00, 100.06it/s] \n",
      "epoch72 loss:4.434340476989746: 100%|██████████| 88/88 [00:00<00:00, 100.16it/s] \n",
      "epoch73 loss:4.331766128540039: 100%|██████████| 88/88 [00:00<00:00, 100.17it/s] \n",
      "epoch74 loss:4.20943546295166: 100%|██████████| 88/88 [00:00<00:00, 100.40it/s]  \n",
      "epoch75 loss:4.280125617980957: 100%|██████████| 88/88 [00:00<00:00, 100.10it/s] \n",
      "epoch76 loss:4.43431282043457: 100%|██████████| 88/88 [00:00<00:00, 100.25it/s]  \n",
      "epoch77 loss:4.032069206237793: 100%|██████████| 88/88 [00:00<00:00, 100.23it/s] \n",
      "epoch78 loss:3.9546210765838623: 100%|██████████| 88/88 [00:00<00:00, 100.24it/s]\n",
      "epoch79 loss:3.779679298400879: 100%|██████████| 88/88 [00:00<00:00, 100.01it/s] \n",
      "epoch80 loss:3.727229356765747: 100%|██████████| 88/88 [00:00<00:00, 100.21it/s] \n",
      "epoch81 loss:3.722832441329956: 100%|██████████| 88/88 [00:00<00:00, 99.98it/s]  \n",
      "epoch82 loss:3.7861833572387695: 100%|██████████| 88/88 [00:00<00:00, 99.85it/s] \n",
      "epoch83 loss:3.7185258865356445: 100%|██████████| 88/88 [00:00<00:00, 100.22it/s]\n",
      "epoch84 loss:3.7427124977111816: 100%|██████████| 88/88 [00:00<00:00, 99.98it/s] \n",
      "epoch85 loss:3.653887987136841: 100%|██████████| 88/88 [00:00<00:00, 99.37it/s] \n",
      "epoch86 loss:3.526813268661499: 100%|██████████| 88/88 [00:00<00:00, 100.11it/s] \n",
      "epoch87 loss:3.5190742015838623: 100%|██████████| 88/88 [00:00<00:00, 107.71it/s]\n",
      "epoch88 loss:3.770151376724243: 100%|██████████| 88/88 [00:00<00:00, 114.69it/s] \n",
      "epoch89 loss:3.3748552799224854: 100%|██████████| 88/88 [00:00<00:00, 116.21it/s]\n",
      "epoch90 loss:3.253098964691162: 100%|██████████| 88/88 [00:00<00:00, 115.32it/s] \n",
      "epoch91 loss:3.296142816543579: 100%|██████████| 88/88 [00:00<00:00, 106.92it/s] \n",
      "epoch92 loss:3.2491323947906494: 100%|██████████| 88/88 [00:00<00:00, 101.57it/s]\n",
      "epoch93 loss:3.102271318435669: 100%|██████████| 88/88 [00:00<00:00, 101.40it/s] \n",
      "epoch94 loss:3.053633213043213: 100%|██████████| 88/88 [00:00<00:00, 101.35it/s] \n",
      "epoch95 loss:3.1006858348846436: 100%|██████████| 88/88 [00:00<00:00, 101.49it/s]\n",
      "epoch96 loss:2.9716248512268066: 100%|██████████| 88/88 [00:00<00:00, 102.61it/s]\n",
      "epoch97 loss:3.244518518447876: 100%|██████████| 88/88 [00:00<00:00, 102.63it/s] \n",
      "epoch98 loss:2.9433062076568604: 100%|██████████| 88/88 [00:00<00:00, 102.77it/s]\n",
      "epoch99 loss:3.104902744293213: 100%|██████████| 88/88 [00:00<00:00, 101.58it/s] \n",
      "epoch100 loss:2.799607276916504: 100%|██████████| 88/88 [00:00<00:00, 101.60it/s] \n",
      "epoch101 loss:2.6517183780670166: 100%|██████████| 88/88 [00:00<00:00, 101.15it/s]\n",
      "epoch102 loss:2.555650234222412: 100%|██████████| 88/88 [00:00<00:00, 100.52it/s] \n",
      "epoch103 loss:2.565450668334961: 100%|██████████| 88/88 [00:00<00:00, 100.50it/s] \n",
      "epoch104 loss:2.601712226867676: 100%|██████████| 88/88 [00:00<00:00, 100.60it/s] \n",
      "epoch105 loss:2.4477999210357666: 100%|██████████| 88/88 [00:00<00:00, 100.45it/s]\n",
      "epoch106 loss:2.9198458194732666: 100%|██████████| 88/88 [00:00<00:00, 100.45it/s]\n",
      "epoch107 loss:2.7064266204833984: 100%|██████████| 88/88 [00:00<00:00, 100.29it/s]\n",
      "epoch108 loss:3.1180641651153564: 100%|██████████| 88/88 [00:00<00:00, 100.46it/s]\n",
      "epoch109 loss:2.7259633541107178: 100%|██████████| 88/88 [00:00<00:00, 100.88it/s]\n",
      "epoch110 loss:3.0684118270874023: 100%|██████████| 88/88 [00:00<00:00, 101.15it/s]\n",
      "epoch111 loss:2.6333630084991455: 100%|██████████| 88/88 [00:00<00:00, 99.88it/s] \n",
      "epoch112 loss:2.6096749305725098: 100%|██████████| 88/88 [00:00<00:00, 99.98it/s] \n",
      "epoch113 loss:2.3828413486480713: 100%|██████████| 88/88 [00:00<00:00, 100.68it/s]\n",
      "epoch114 loss:2.2694077491760254: 100%|██████████| 88/88 [00:00<00:00, 100.68it/s]\n",
      "epoch115 loss:2.2606329917907715: 100%|██████████| 88/88 [00:00<00:00, 100.18it/s]\n",
      "epoch116 loss:2.097052812576294: 100%|██████████| 88/88 [00:00<00:00, 100.35it/s] \n",
      "epoch117 loss:2.0981955528259277: 100%|██████████| 88/88 [00:00<00:00, 99.75it/s] \n",
      "epoch118 loss:2.094212770462036: 100%|██████████| 88/88 [00:00<00:00, 99.93it/s]  \n",
      "epoch119 loss:1.933157205581665: 100%|██████████| 88/88 [00:00<00:00, 99.83it/s]  \n",
      "epoch120 loss:1.8926719427108765: 100%|██████████| 88/88 [00:00<00:00, 99.35it/s]\n",
      "epoch121 loss:1.9279850721359253: 100%|██████████| 88/88 [00:00<00:00, 99.82it/s]\n",
      "epoch122 loss:1.9808269739151: 100%|██████████| 88/88 [00:00<00:00, 100.42it/s]   \n",
      "epoch123 loss:1.8491499423980713: 100%|██████████| 88/88 [00:00<00:00, 100.20it/s]\n",
      "epoch124 loss:1.6515700817108154: 100%|██████████| 88/88 [00:00<00:00, 100.24it/s]\n",
      "epoch125 loss:1.6997771263122559: 100%|██████████| 88/88 [00:00<00:00, 100.38it/s]\n",
      "epoch126 loss:1.5422618389129639: 100%|██████████| 88/88 [00:00<00:00, 99.95it/s] \n",
      "epoch127 loss:1.3838809728622437: 100%|██████████| 88/88 [00:00<00:00, 97.48it/s] \n",
      "epoch128 loss:1.4067095518112183: 100%|██████████| 88/88 [00:00<00:00, 100.13it/s]\n",
      "epoch129 loss:1.3017915487289429: 100%|██████████| 88/88 [00:00<00:00, 100.13it/s]\n",
      "epoch130 loss:1.4513903856277466: 100%|██████████| 88/88 [00:00<00:00, 99.97it/s] \n",
      "epoch131 loss:1.7371083498001099: 100%|██████████| 88/88 [00:00<00:00, 100.29it/s]\n",
      "epoch132 loss:1.502524733543396: 100%|██████████| 88/88 [00:00<00:00, 100.58it/s] \n",
      "epoch133 loss:1.4245840311050415: 100%|██████████| 88/88 [00:00<00:00, 100.64it/s]\n",
      "epoch134 loss:1.2862255573272705: 100%|██████████| 88/88 [00:00<00:00, 100.37it/s]\n",
      "epoch135 loss:1.3425703048706055: 100%|██████████| 88/88 [00:00<00:00, 100.54it/s]\n",
      "epoch136 loss:1.3405555486679077: 100%|██████████| 88/88 [00:00<00:00, 100.53it/s]\n",
      "epoch137 loss:1.4405113458633423: 100%|██████████| 88/88 [00:00<00:00, 100.80it/s]\n",
      "epoch138 loss:1.8827732801437378: 100%|██████████| 88/88 [00:00<00:00, 100.31it/s]\n",
      "epoch139 loss:1.5489192008972168: 100%|██████████| 88/88 [00:00<00:00, 100.58it/s]\n",
      "epoch140 loss:1.854790210723877: 100%|██████████| 88/88 [00:00<00:00, 100.52it/s] \n",
      "epoch141 loss:1.7479791641235352: 100%|██████████| 88/88 [00:00<00:00, 100.66it/s]\n",
      "epoch142 loss:1.6512118577957153: 100%|██████████| 88/88 [00:00<00:00, 100.44it/s]\n",
      "epoch143 loss:1.5651401281356812: 100%|██████████| 88/88 [00:00<00:00, 100.53it/s]\n",
      "epoch144 loss:1.5688660144805908: 100%|██████████| 88/88 [00:00<00:00, 100.56it/s]\n",
      "epoch145 loss:1.4786165952682495: 100%|██████████| 88/88 [00:00<00:00, 100.59it/s]\n",
      "epoch146 loss:1.4942668676376343: 100%|██████████| 88/88 [00:00<00:00, 100.57it/s]\n",
      "epoch147 loss:1.39508056640625: 100%|██████████| 88/88 [00:00<00:00, 100.66it/s]  \n",
      "epoch148 loss:1.458407998085022: 100%|██████████| 88/88 [00:00<00:00, 100.00it/s] \n",
      "epoch149 loss:1.2404935359954834: 100%|██████████| 88/88 [00:00<00:00, 100.29it/s]\n",
      "epoch150 loss:1.210679054260254: 100%|██████████| 88/88 [00:00<00:00, 100.43it/s] \n",
      "epoch151 loss:1.0415139198303223: 100%|██████████| 88/88 [00:00<00:00, 100.46it/s]\n",
      "epoch152 loss:0.9903122186660767: 100%|██████████| 88/88 [00:00<00:00, 100.48it/s]\n",
      "epoch153 loss:0.9522072076797485: 100%|██████████| 88/88 [00:00<00:00, 100.26it/s]\n",
      "epoch154 loss:0.9686992764472961: 100%|██████████| 88/88 [00:00<00:00, 100.25it/s]\n",
      "epoch155 loss:0.8398130536079407: 100%|██████████| 88/88 [00:00<00:00, 100.44it/s]\n",
      "epoch156 loss:0.7491029500961304: 100%|██████████| 88/88 [00:00<00:00, 100.11it/s]\n",
      "epoch157 loss:0.8376396894454956: 100%|██████████| 88/88 [00:00<00:00, 100.36it/s]\n",
      "epoch158 loss:1.1265331506729126: 100%|██████████| 88/88 [00:00<00:00, 100.09it/s]\n",
      "epoch159 loss:1.1499744653701782: 100%|██████████| 88/88 [00:00<00:00, 100.49it/s]\n",
      "epoch160 loss:0.9588541388511658: 100%|██████████| 88/88 [00:00<00:00, 100.27it/s]\n",
      "epoch161 loss:1.098585605621338: 100%|██████████| 88/88 [00:00<00:00, 100.19it/s] \n",
      "epoch162 loss:1.2723852396011353: 100%|██████████| 88/88 [00:00<00:00, 100.26it/s]\n",
      "epoch163 loss:1.0251250267028809: 100%|██████████| 88/88 [00:00<00:00, 100.23it/s]\n",
      "epoch164 loss:1.3546408414840698: 100%|██████████| 88/88 [00:00<00:00, 100.48it/s]\n",
      "epoch165 loss:1.0673388242721558: 100%|██████████| 88/88 [00:00<00:00, 100.44it/s]\n",
      "epoch166 loss:1.211329698562622: 100%|██████████| 88/88 [00:00<00:00, 100.39it/s] \n",
      "epoch167 loss:1.1306090354919434: 100%|██████████| 88/88 [00:00<00:00, 100.38it/s]\n",
      "epoch168 loss:0.9179959893226624: 100%|██████████| 88/88 [00:00<00:00, 100.30it/s]\n",
      "epoch169 loss:1.420088529586792: 100%|██████████| 88/88 [00:00<00:00, 100.08it/s] \n",
      "epoch170 loss:0.7866113185882568: 100%|██████████| 88/88 [00:00<00:00, 100.39it/s]\n",
      "epoch171 loss:0.7458238005638123: 100%|██████████| 88/88 [00:00<00:00, 100.28it/s]\n",
      "epoch172 loss:0.8373892307281494: 100%|██████████| 88/88 [00:00<00:00, 100.32it/s]\n",
      "epoch173 loss:0.9529843330383301: 100%|██████████| 88/88 [00:00<00:00, 100.39it/s]\n",
      "epoch174 loss:0.7995229959487915: 100%|██████████| 88/88 [00:00<00:00, 100.52it/s]\n",
      "epoch175 loss:0.6816506385803223: 100%|██████████| 88/88 [00:00<00:00, 100.50it/s]\n",
      "epoch176 loss:0.5569310188293457: 100%|██████████| 88/88 [00:00<00:00, 100.28it/s]\n",
      "epoch177 loss:0.5469241142272949: 100%|██████████| 88/88 [00:00<00:00, 100.46it/s]\n",
      "epoch178 loss:0.624474287033081: 100%|██████████| 88/88 [00:00<00:00, 100.25it/s] \n",
      "epoch179 loss:0.6261609792709351: 100%|██████████| 88/88 [00:00<00:00, 100.05it/s]\n",
      "epoch180 loss:0.6340288519859314: 100%|██████████| 88/88 [00:00<00:00, 100.64it/s]\n",
      "epoch181 loss:0.5575188994407654: 100%|██████████| 88/88 [00:00<00:00, 100.42it/s]\n",
      "epoch182 loss:0.5322625637054443: 100%|██████████| 88/88 [00:00<00:00, 100.34it/s]\n",
      "epoch183 loss:0.5316533446311951: 100%|██████████| 88/88 [00:00<00:00, 100.50it/s]\n",
      "epoch184 loss:0.48048728704452515: 100%|██████████| 88/88 [00:00<00:00, 100.38it/s]\n",
      "epoch185 loss:0.7381033897399902: 100%|██████████| 88/88 [00:00<00:00, 100.53it/s]\n",
      "epoch186 loss:0.7817311882972717: 100%|██████████| 88/88 [00:00<00:00, 100.39it/s]\n",
      "epoch187 loss:0.8515540361404419: 100%|██████████| 88/88 [00:00<00:00, 100.55it/s]\n",
      "epoch188 loss:0.5272240042686462: 100%|██████████| 88/88 [00:00<00:00, 100.43it/s]\n",
      "epoch189 loss:0.5101585388183594: 100%|██████████| 88/88 [00:00<00:00, 100.46it/s]\n",
      "epoch190 loss:0.48931685090065: 100%|██████████| 88/88 [00:00<00:00, 100.46it/s]  \n",
      "epoch191 loss:0.3967006206512451: 100%|██████████| 88/88 [00:00<00:00, 100.29it/s]\n",
      "epoch192 loss:0.3584916591644287: 100%|██████████| 88/88 [00:00<00:00, 99.97it/s] \n",
      "epoch193 loss:0.41706475615501404: 100%|██████████| 88/88 [00:00<00:00, 100.43it/s]\n",
      "epoch194 loss:0.34156325459480286: 100%|██████████| 88/88 [00:00<00:00, 100.50it/s]\n",
      "epoch195 loss:0.9124137163162231: 100%|██████████| 88/88 [00:00<00:00, 100.20it/s]\n",
      "epoch196 loss:0.44378823041915894: 100%|██████████| 88/88 [00:00<00:00, 100.30it/s]\n",
      "epoch197 loss:0.3108610212802887: 100%|██████████| 88/88 [00:00<00:00, 100.47it/s]\n",
      "epoch198 loss:0.3252447247505188: 100%|██████████| 88/88 [00:00<00:00, 100.63it/s]\n",
      "epoch199 loss:0.3809148371219635: 100%|██████████| 88/88 [00:00<00:00, 111.49it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim.adam import Adam\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dataset = TextGeneration()\n",
    "model = LSTM(num_embeddings = len(dataset.BOW)).to(device)\n",
    "loader = DataLoader(dataset, batch_size=64)\n",
    "optim = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(200):\n",
    "    iterator = tqdm.tqdm(loader)\n",
    "    for data, label in iterator:\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        pred = model(torch.tensor(data, dtype=torch.long).to(device))\n",
    "        \n",
    "        loss = nn.CrossEntropyLoss()(pred, torch.tensor(label, dtype=torch.long).to(device))\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        iterator.set_description(f\"epoch{epoch} loss:{loss.item()}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"lstm.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 과정에서는 model이 입력받은 데이터 다음으로 올 단어가 무엇일지 예측하는 pred를 반환하고, 이후 label과의 CrossEntropyLoss를 계산하여 backpropagation을 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word: finding an \n",
      "predicted sentence: finding an affects bill to autumn vote like recommends an cars some \n"
     ]
    }
   ],
   "source": [
    "def generate(model, BOW, string=\"finding an \", strlen=10):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    print(f\"input word: {string}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for p in range(strlen):\n",
    "            words = torch.tensor([BOW[w] for w in string.split()], dtype=torch.long).to(device)\n",
    "\n",
    "            input_tensor = torch.unsqueeze(words[-2:], dim=0)\n",
    "            output = model(input_tensor)\n",
    "            output_word = (torch.argmax(output).cpu().numpy())\n",
    "            string += list(BOW.keys())[output_word]\n",
    "            string += \" \"\n",
    "    \n",
    "    print(f\"predicted sentence: {string}\")\n",
    "    \n",
    "model.load_state_dict(torch.load(\"lstm.pth\", map_location=device))\n",
    "pred = generate(model, dataset.BOW)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
